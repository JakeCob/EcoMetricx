{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EcoMetricx: Complete Processing Pipeline\n",
        "\n",
        "**A Comprehensive End-to-End Document Intelligence System**\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Complete Pipeline Overview\n",
        "\n",
        "Welcome to **EcoMetricx** - the complete advanced PDF processing and query system. This notebook demonstrates the entire pipeline from raw PDF documents to intelligent search and retrieval:\n",
        "\n",
        "### 📚 **Part 1: Enhanced PDF Processing & Text Extraction**\n",
        "- 📄 **Extract text** from PDFs using multiple intelligent methods\n",
        "- 👁️ **Process visual content** exactly as humans see it \n",
        "- 🔍 **Compare extraction methods** with comprehensive analysis\n",
        "- 📊 **Generate organized outputs** with metadata and tracking\n",
        "\n",
        "### 🔍 **Part 2: Advanced Visual Element Extraction**\n",
        "- 📊 **Identify and extract** tables, charts, and images automatically\n",
        "- 🤖 **Apply AI classification** with text correlation analysis\n",
        "- ✨ **Enhance images** for optimal AI processing\n",
        "- 🗂️ **Organize by type and visibility** with rich metadata\n",
        "\n",
        "### 🎯 **Part 3: Intelligent Query & Retrieval System**\n",
        "- 🔧 **Build search indices** with TF-IDF and embeddings\n",
        "- 🗄️ **Database integration** with Postgres and vector search\n",
        "- 🌐 **API development** for production query systems  \n",
        "- 📈 **Interactive queries** with real-time search capabilities\n",
        "\n",
        "### Why This Complete Solution Matters\n",
        "\n",
        "Traditional document processing fails because it treats each step in isolation. Our **integrated approach** provides:\n",
        "- **End-to-end consistency** across all processing stages\n",
        "- **Query-optimized outputs** from the very beginning\n",
        "- **Multi-modal understanding** combining text, images, and structure\n",
        "- **Production-ready formats** for immediate deployment\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ System Architecture\n",
        "\n",
        "The EcoMetricx pipeline consists of three integrated phases:\n",
        "\n",
        "```\n",
        "📄 Raw PDF → 🔍 Enhanced Processing → 👁️ Visual Analysis → 🎯 Query System\n",
        "     ↓              ↓                     ↓                ↓\n",
        "   Input        Text & Layout        Tables & Images    Search & API\n",
        "```\n",
        "\n",
        "Let's begin with **Part 1: Enhanced PDF Processing**...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 📚 Part 1: Enhanced PDF Processing & Text Extraction\n",
        "\n",
        "## 🛠️ Environment Setup\n",
        "\n",
        "Let's start by setting up our development environment. Think of this as preparing your complete toolbox for the entire EcoMetricx pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🐍 Python Version: 3.11.13 | packaged by conda-forge | (main, Jun  4 2025, 14:48:23) [GCC 13.3.0]\n",
            "💻 Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
            "📁 Python Executable: /root/anaconda3/envs/pdf-extractor/bin/python\n",
            "🌐 Conda Environment: pdf-extractor\n"
          ]
        }
      ],
      "source": [
        "### Step 1: Check Your Python Environment\n",
        "\n",
        "# First, let's verify we're using the correct Python environment\n",
        "import sys\n",
        "import platform\n",
        "print(f\"🐍 Python Version: {sys.version}\")\n",
        "print(f\"💻 Platform: {platform.platform()}\")\n",
        "print(f\"📁 Python Executable: {sys.executable}\")\n",
        "\n",
        "# Check if we're in the correct conda environment\n",
        "import os\n",
        "conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'Not in conda environment')\n",
        "print(f\"🌐 Conda Environment: {conda_env}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ pdf2image imported successfully\n",
            "✅ pdfplumber version: 0.11.7\n",
            "✅ pytesseract imported successfully\n",
            "✅ OpenCV version: 4.12.0\n",
            "✅ Pillow (PIL) version: 11.3.0\n",
            "✅ scikit-image version: 0.25.2\n",
            "✅ Pandas version: 2.3.2\n",
            "✅ Matplotlib version: 3.10.6\n",
            "\n",
            "🎯 Core libraries loaded for complete EcoMetricx pipeline!\n"
          ]
        }
      ],
      "source": [
        "### Step 2: Install Required Libraries\n",
        "\n",
        "# Core Python libraries\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# PDF and OCR processing\n",
        "try:\n",
        "    import pdf2image\n",
        "    print(f\"✅ pdf2image imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ pdf2image import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    version = getattr(pdfplumber, '__version__', 'unknown version')\n",
        "    print(f\"✅ pdfplumber version: {version}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ pdfplumber import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import pytesseract\n",
        "    print(f\"✅ pytesseract imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ pytesseract import failed: {e}\")\n",
        "\n",
        "# Image processing\n",
        "try:\n",
        "    import cv2\n",
        "    print(f\"✅ OpenCV version: {cv2.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ OpenCV import failed (will use PIL fallback): {e}\")\n",
        "\n",
        "try:\n",
        "    from PIL import Image, ImageEnhance\n",
        "    try:\n",
        "        version = Image.__version__\n",
        "    except AttributeError:\n",
        "        import PIL\n",
        "        version = getattr(PIL, '__version__', 'unknown version')\n",
        "    print(f\"✅ Pillow (PIL) version: {version}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Pillow import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import skimage\n",
        "    print(f\"✅ scikit-image version: {skimage.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ scikit-image import failed: {e}\")\n",
        "\n",
        "# Data processing and visualization\n",
        "try:\n",
        "    import pandas as pd\n",
        "    print(f\"✅ Pandas version: {pd.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Pandas import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib\n",
        "    print(f\"✅ Matplotlib version: {matplotlib.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"❌ Matplotlib import failed: {e}\")\n",
        "\n",
        "print(\"\\n🎯 Core libraries loaded for complete EcoMetricx pipeline!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Enhanced Output Management System Initialized\n",
            "============================================================\n",
            "📁 Session ID: 20250903_232652\n",
            "📅 Session created: 2025-09-03T23:26:52.368159\n",
            "📂 Base output directory: output\n",
            "\n",
            "🏗️ Organized Directory Structure Created:\n",
            "   📊 Session metadata and tracking\n",
            "   📋 Document-based organization\n",
            "   🔍 Method-specific subdirectories\n",
            "   🗄️ Query-optimized formats\n",
            "   📈 Comprehensive metadata generation\n",
            "\n",
            "💡 Key Benefits:\n",
            "   ✅ Eliminates messy file organization\n",
            "   ✅ Provides clear file naming with timestamps\n",
            "   ✅ Separates extraction methods properly\n",
            "   ✅ Generates query-ready formats automatically\n",
            "   ✅ Maintains rich metadata for search systems\n",
            "\n",
            "✅ Enhanced output system ready for complete pipeline!\n"
          ]
        }
      ],
      "source": [
        "### Step 3: Initialize Enhanced Output Management System\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd()\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "# Initialize the enhanced output management system\n",
        "from src.core.output_manager import get_output_manager\n",
        "output_manager = get_output_manager()\n",
        "\n",
        "print(\"🎯 Enhanced Output Management System Initialized\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📁 Session ID: {output_manager.session_id}\")\n",
        "print(f\"📅 Session created: {output_manager.session_timestamp}\")\n",
        "print(f\"📂 Base output directory: {output_manager.base_dir}\")\n",
        "\n",
        "print(f\"\\n🏗️ Organized Directory Structure Created:\")\n",
        "print(f\"   📊 Session metadata and tracking\")\n",
        "print(f\"   📋 Document-based organization\") \n",
        "print(f\"   🔍 Method-specific subdirectories\")\n",
        "print(f\"   🗄️ Query-optimized formats\")\n",
        "print(f\"   📈 Comprehensive metadata generation\")\n",
        "\n",
        "print(f\"\\n💡 Key Benefits:\")\n",
        "print(f\"   ✅ Eliminates messy file organization\")\n",
        "print(f\"   ✅ Provides clear file naming with timestamps\")\n",
        "print(f\"   ✅ Separates extraction methods properly\")\n",
        "print(f\"   ✅ Generates query-ready formats automatically\")\n",
        "print(f\"   ✅ Maintains rich metadata for search systems\")\n",
        "\n",
        "print(f\"\\n✅ Enhanced output system ready for complete pipeline!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-03 23:26:52,385 - EcoMetricx_Pipeline - INFO - 🚀 EcoMetricx Complete Pipeline logging initialized\n",
            "2025-09-03 23:26:52,498 - EcoMetricx_Pipeline - INFO - ✅ Tesseract OCR version: 4.1.1\n",
            "2025-09-03 23:26:52,608 - EcoMetricx_Pipeline - INFO - ✅ OCR test completed successfully\n",
            "🔧 OCR Configuration:\n",
            "  tesseract_config: --oem 3 --psm 6 -c tessedit_char_blacklist=\n",
            "  confidence_threshold: 60\n",
            "  dpi: 300\n",
            "  preprocessing: True\n",
            "\n",
            "✅ System configuration complete!\n"
          ]
        }
      ],
      "source": [
        "### Step 4: Configure Logging and OCR\n",
        "\n",
        "# Configure logging for our demonstration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(sys.stdout),  # Display logs in notebook\n",
        "        logging.FileHandler(output_manager.base_dir / 'session_metadata' / 'pipeline_log.txt')\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger('EcoMetricx_Pipeline')\n",
        "logger.info(\"🚀 EcoMetricx Complete Pipeline logging initialized\")\n",
        "\n",
        "# OCR Configuration\n",
        "OCR_CONFIG = {\n",
        "    'tesseract_config': '--oem 3 --psm 6 -c tessedit_char_blacklist=',\n",
        "    'confidence_threshold': 60,\n",
        "    'dpi': 300,\n",
        "    'preprocessing': True\n",
        "}\n",
        "\n",
        "# Test OCR installation\n",
        "try:\n",
        "    version = pytesseract.get_tesseract_version()\n",
        "    logger.info(f\"✅ Tesseract OCR version: {version}\")\n",
        "    \n",
        "    # Test with a simple image\n",
        "    test_image = Image.new('RGB', (200, 50), color='white')\n",
        "    test_result = pytesseract.image_to_string(test_image)\n",
        "    logger.info(\"✅ OCR test completed successfully\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"❌ OCR configuration failed: {e}\")\n",
        "\n",
        "print(\"🔧 OCR Configuration:\")\n",
        "for key, value in OCR_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(f\"\\n✅ System configuration complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced PDF Extractor imported successfully\n",
            "✅ Visual PDF Extractor imported successfully\n",
            "📄 Loading Test PDF for Complete Pipeline Demo\n",
            "==================================================\n",
            "✅ Found test PDF: test_info_extract.pdf\n",
            "📁 File location: /root/Programming Projects/Personal/EcoMetricx/task/test_info_extract.pdf\n",
            "📊 File size: 114.1 KB\n",
            "📖 Number of pages: 2\n",
            "📐 Page dimensions: 612 x 792 points\n",
            "\n",
            "🎯 This PDF will be processed through the complete pipeline:\n",
            "   • Enhanced text extraction with layout analysis\n",
            "   • Visual OCR extraction with confidence scoring\n",
            "   • Advanced visual element detection and extraction\n",
            "   • Enhanced image processing with AI classification\n",
            "   • Query-ready format generation\n",
            "   • Database-ready structured outputs\n",
            "\n",
            "✅ Extraction engines and test data ready for complete pipeline!\n"
          ]
        }
      ],
      "source": [
        "### Step 5: Load Extraction Engines and Test PDF\n",
        "\n",
        "# Import our custom extraction classes\n",
        "try:\n",
        "    from src.extractors.enhanced_pdf_extractor import EnhancedPDFTextExtractor\n",
        "    print(\"✅ Enhanced PDF Extractor imported successfully\")\n",
        "    enhanced_extractor = EnhancedPDFTextExtractor()\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Enhanced PDF Extractor not found: {e}\")\n",
        "    enhanced_extractor = None\n",
        "\n",
        "try:\n",
        "    from src.extractors.visual_pdf_extractor import VisualPDFExtractor, HybridPDFExtractor\n",
        "    print(\"✅ Visual PDF Extractor imported successfully\")\n",
        "    visual_extractor = VisualPDFExtractor()\n",
        "    hybrid_extractor = HybridPDFExtractor()\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Visual PDF Extractor not found: {e}\")\n",
        "    visual_extractor = None\n",
        "    hybrid_extractor = None\n",
        "\n",
        "# Load test PDF document\n",
        "demo_pdf = project_root / \"task\" / \"test_info_extract.pdf\"\n",
        "\n",
        "print(\"📄 Loading Test PDF for Complete Pipeline Demo\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if demo_pdf.exists():\n",
        "    print(f\"✅ Found test PDF: {demo_pdf.name}\")\n",
        "    print(f\"📁 File location: {demo_pdf}\")\n",
        "    print(f\"📊 File size: {demo_pdf.stat().st_size / 1024:.1f} KB\")\n",
        "    \n",
        "    # Get basic info about the PDF\n",
        "    try:\n",
        "        import fitz  # PyMuPDF\n",
        "        with fitz.open(str(demo_pdf)) as doc:\n",
        "            page_count = len(doc)\n",
        "            print(f\"📖 Number of pages: {page_count}\")\n",
        "            \n",
        "            # Get first page dimensions\n",
        "            first_page = doc[0]\n",
        "            rect = first_page.rect\n",
        "            print(f\"📐 Page dimensions: {rect.width:.0f} x {rect.height:.0f} points\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"ℹ️ Could not read PDF metadata: {e}\")\n",
        "        \n",
        "    print(\"\\n🎯 This PDF will be processed through the complete pipeline:\")\n",
        "    print(\"   • Enhanced text extraction with layout analysis\")\n",
        "    print(\"   • Visual OCR extraction with confidence scoring\") \n",
        "    print(\"   • Advanced visual element detection and extraction\")\n",
        "    print(\"   • Enhanced image processing with AI classification\")\n",
        "    print(\"   • Query-ready format generation\")\n",
        "    print(\"   • Database-ready structured outputs\")\n",
        "    \n",
        "else:\n",
        "    print(f\"❌ Test PDF not found at: {demo_pdf}\")\n",
        "    print(\"💡 Please ensure the test_info_extract.pdf file exists in the 'task' directory\")\n",
        "\n",
        "print(f\"\\n✅ Extraction engines and test data ready for complete pipeline!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Enhanced PDF Text Extraction\n",
        "\n",
        "The **Enhanced PDF Extractor** uses programmatic methods to extract text directly from the PDF's internal structure. This is lightning-fast and highly accurate for text-based PDFs.\n",
        "\n",
        "### How it works:\n",
        "1. **PyMuPDF**: Extracts text from PDF objects directly\n",
        "2. **PDFPlumber**: Analyzes layout and table structures  \n",
        "3. **Smart Fallback**: If one method fails, automatically tries others\n",
        "4. **Quality Assessment**: Scores extraction results to pick the best method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Enhanced PDF Extraction...\n",
            "==================================================\n",
            "📋 Document registered: test_info_extract\n",
            "2025-09-03 23:26:52,750 - src.extractors.enhanced_pdf_extractor - INFO - Starting enhanced extraction from: /root/Programming Projects/Personal/EcoMetricx/task/test_info_extract.pdf\n",
            "2025-09-03 23:26:53,192 - src.extractors.enhanced_pdf_extractor - INFO - Successfully extracted text from 2 pages\n",
            "⏱️ Extraction completed in 0.44 seconds\n",
            "📊 Text length: 1504 characters\n",
            "🎯 Method used: Enhanced Layout Analysis\n",
            "📄 Pages processed: 2\n",
            "\n",
            "💾 Files saved with organized structure:\n",
            "   📄 Full Text: test_info_extract_20250903_232653_full_text.txt\n",
            "   📄 Structured Data: test_info_extract_20250903_232653_structured_data.json\n",
            "   📄 Layout Analysis: test_info_extract_20250903_232653_layout_analysis.json\n",
            "   📄 Extraction Report: test_info_extract_20250903_232653_extraction_report.json\n",
            "\n",
            "📄 First 500 characters of extracted text:\n",
            "--------------------------------------------------\n",
            "## Home Energy Report: **electricity**\n",
            "\n",
            "#### March report Account number: 954137 Service address: 1627 Tulip Lane\n",
            "\n",
            "|Find your personalized<br>analysis of your electrical<br>energy use. Scan this code<br>or log in to your account at<br>franklinenergy.com.\n",
            "|---|---|---|---|---|\n",
            "|Find your personalized<br>analysis of your electrical<br>energy use. Scan this code<br>or log in to your account at<br>**franklinenergy.com**.\n",
            "\n",
            "#### Dear JILL DOE, here is your usage analysis for March.\n",
            "\n",
            "#### Your electric\n",
            "\n",
            "... [1004 more characters]\n",
            "\n",
            "📊 Layout Analysis Results:\n",
            "   • Pages analyzed: 2\n",
            "   • Columns detected: 3\n",
            "   • Tables found: 1\n",
            "   • Headers identified: 16\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate Enhanced PDF Extraction with Organized Output\n",
        "if enhanced_extractor and demo_pdf.exists():\n",
        "    print(\"🚀 Starting Enhanced PDF Extraction...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Register document with output manager\n",
        "        document_id = output_manager.register_document(str(demo_pdf))\n",
        "        print(f\"📋 Document registered: {document_id}\")\n",
        "        \n",
        "        # Time the extraction process\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Extract text using the enhanced method\n",
        "        enhanced_result = enhanced_extractor.extract_with_layout_analysis(str(demo_pdf), preserve_structure=True)\n",
        "        \n",
        "        extraction_time = time.time() - start_time\n",
        "        enhanced_result['processing_time'] = extraction_time\n",
        "        \n",
        "        print(f\"⏱️ Extraction completed in {extraction_time:.2f} seconds\")\n",
        "        print(f\"📊 Text length: {len(enhanced_result.get('full_text', ''))} characters\")\n",
        "        print(f\"🎯 Method used: Enhanced Layout Analysis\")\n",
        "        print(f\"📄 Pages processed: {enhanced_result.get('total_pages', 'N/A')}\")\n",
        "        \n",
        "        # Save results using output manager\n",
        "        saved_files = output_manager.save_enhanced_pdf_extraction(document_id, enhanced_result)\n",
        "        \n",
        "        print(f\"\\n💾 Files saved with organized structure:\")\n",
        "        for file_type, file_path in saved_files.items():\n",
        "            file_name = Path(file_path).name\n",
        "            print(f\"   📄 {file_type.replace('_', ' ').title()}: {file_name}\")\n",
        "        \n",
        "        # Display first 500 characters of extracted text\n",
        "        extracted_text = enhanced_result.get('full_text', '')\n",
        "        if extracted_text:\n",
        "            print(f\"\\n📄 First 500 characters of extracted text:\")\n",
        "            print(\"-\" * 50)\n",
        "            print(extracted_text[:500])\n",
        "            if len(extracted_text) > 500:\n",
        "                print(f\"\\n... [{len(extracted_text) - 500} more characters]\")\n",
        "        \n",
        "        # Show layout analysis results\n",
        "        layout_analysis = enhanced_result.get('layout_analysis', [])\n",
        "        if layout_analysis:\n",
        "            print(f\"\\n📊 Layout Analysis Results:\")\n",
        "            total_columns = sum(len(page.get('columns', [])) for page in layout_analysis)\n",
        "            total_tables = sum(len(page.get('tables', [])) for page in layout_analysis) \n",
        "            total_headers = sum(len(page.get('headers', [])) for page in layout_analysis)\n",
        "            \n",
        "            print(f\"   • Pages analyzed: {len(layout_analysis)}\")\n",
        "            print(f\"   • Columns detected: {total_columns}\")\n",
        "            print(f\"   • Tables found: {total_tables}\")\n",
        "            print(f\"   • Headers identified: {total_headers}\")\n",
        "        \n",
        "        # Store result for comparison\n",
        "        enhanced_text = extracted_text\n",
        "        enhanced_stats = {\n",
        "            'method': 'Enhanced Layout Analysis',\n",
        "            'time': extraction_time,\n",
        "            'length': len(extracted_text),\n",
        "            'confidence': 95,\n",
        "            'document_id': document_id,\n",
        "            'files_saved': len(saved_files)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Enhanced extraction failed: {str(e)}\")\n",
        "        enhanced_text = \"\"\n",
        "        enhanced_stats = {'method': 'Enhanced', 'time': 0, 'length': 0, 'confidence': 0}\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ Enhanced extractor not available - showing example results\")\n",
        "    enhanced_text = \"Example enhanced extraction results would appear here...\"\n",
        "    enhanced_stats = {'method': 'Enhanced', 'time': 0.15, 'length': 1504, 'confidence': 95}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 👁️ Visual PDF Extraction (OCR)\n",
        "\n",
        "The **Visual PDF Extractor** takes screenshots of PDF pages and uses OCR to read the text. This method sees exactly what a human would see when looking at the document.\n",
        "\n",
        "### How it works:\n",
        "1. **PDF to Image**: Converts each PDF page to a high-resolution screenshot (300 DPI)\n",
        "2. **Image Preprocessing**: Enhances image quality for better OCR results\n",
        "3. **OCR Processing**: Uses Google's Tesseract engine to read text from images\n",
        "4. **Confidence Scoring**: Measures how confident the OCR is about each word\n",
        "5. **Post-processing**: Cleans and formats the extracted text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "👁️ Starting Visual PDF Extraction...\n",
            "==================================================\n",
            "2025-09-03 23:26:53,203 - src.extractors.visual_pdf_extractor.VisualPDFExtractor - INFO - Starting visual extraction of /root/Programming Projects/Personal/EcoMetricx/task/test_info_extract.pdf\n",
            "2025-09-03 23:26:53,204 - src.extractors.visual_pdf_extractor.VisualPDFExtractor - INFO - Converting PDF to images at 300 DPI\n",
            "2025-09-03 23:26:53,739 - src.extractors.visual_pdf_extractor.VisualPDFExtractor - INFO - Processing page 1/2\n",
            "2025-09-03 23:26:55,223 - src.extractors.visual_pdf_extractor.VisualPDFExtractor - INFO - Processing page 2/2\n",
            "2025-09-03 23:26:56,795 - src.extractors.visual_pdf_extractor.VisualPDFExtractor - INFO - Visual extraction completed. Average confidence: 89.1%\n",
            "⏱️ Extraction completed in 3.59 seconds\n",
            "📊 Text length: 1947 characters\n",
            "🎯 Method: Visual OCR\n",
            "📈 OCR Confidence: 89.1%\n",
            "📄 Pages processed: 2\n",
            "\n",
            "💾 Files saved with organized structure:\n",
            "   📷 Ocr Text: test_info_extract_20250903_232656_ocr_text.txt\n",
            "   📷 Confidence Scores: test_info_extract_20250903_232656_confidence_scores.json\n",
            "   📷 Ocr Report: test_info_extract_20250903_232656_ocr_report.json\n",
            "\n",
            "📄 First 500 characters of extracted text:\n",
            "--------------------------------------------------\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby homes ° a 103 kWh typ | Ca U se Efficient nearby homes hs 49 kWh Nearby homes are defined as... Monthly savings tip: Do full laundry loads. Other homes with electricity Waiting until you > neue = %6 Homes within 9 km have a full load to wy run your laundry LA ca\n",
            "\n",
            "... [1447 more characters]\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate Visual PDF Extraction with Organized Output\n",
        "if visual_extractor and demo_pdf.exists():\n",
        "    print(\"👁️ Starting Visual PDF Extraction...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Use the same document ID (already registered)\n",
        "        document_id = enhanced_stats.get('document_id', output_manager.register_document(str(demo_pdf)))\n",
        "        \n",
        "        # Time the extraction process\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Extract text using the visual method (OCR)\n",
        "        visual_result = visual_extractor.extract_via_screenshot(str(demo_pdf), preprocess=True)\n",
        "        \n",
        "        extraction_time = time.time() - start_time\n",
        "        visual_result['processing_time'] = extraction_time\n",
        "        \n",
        "        print(f\"⏱️ Extraction completed in {extraction_time:.2f} seconds\")\n",
        "        print(f\"📊 Text length: {len(visual_result.get('full_text', ''))} characters\")\n",
        "        print(f\"🎯 Method: Visual OCR\")\n",
        "        print(f\"📈 OCR Confidence: {visual_result.get('average_confidence', 0):.1f}%\")\n",
        "        print(f\"📄 Pages processed: {visual_result.get('total_pages', 0)}\")\n",
        "        \n",
        "        # Save results using output manager\n",
        "        saved_files = output_manager.save_visual_ocr_extraction(document_id, visual_result, [])\n",
        "        \n",
        "        print(f\"\\n💾 Files saved with organized structure:\")\n",
        "        for file_type, file_path in saved_files.items():\n",
        "            file_name = Path(file_path).name\n",
        "            print(f\"   📷 {file_type.replace('_', ' ').title()}: {file_name}\")\n",
        "        \n",
        "        # Display first 500 characters of extracted text\n",
        "        extracted_text = visual_result.get('full_text', '')\n",
        "        if extracted_text:\n",
        "            print(f\"\\n📄 First 500 characters of extracted text:\")\n",
        "            print(\"-\" * 50)\n",
        "            print(extracted_text[:500])\n",
        "            if len(extracted_text) > 500:\n",
        "                print(f\"\\n... [{len(extracted_text) - 500} more characters]\")\n",
        "        \n",
        "        # Store result for comparison\n",
        "        visual_text = extracted_text\n",
        "        visual_stats = {\n",
        "            'method': 'Visual OCR',\n",
        "            'time': extraction_time,\n",
        "            'length': len(extracted_text),\n",
        "            'confidence': visual_result.get('average_confidence', 0),\n",
        "            'document_id': document_id,\n",
        "            'files_saved': len(saved_files)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Visual extraction failed: {str(e)}\")\n",
        "        visual_text = \"\"\n",
        "        visual_stats = {'method': 'Visual OCR', 'time': 0, 'length': 0, 'confidence': 0}\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ Visual extractor not available - showing example results\")\n",
        "    visual_text = \"Example visual OCR extraction results would appear here...\"\n",
        "    visual_stats = {'method': 'Visual OCR', 'time': 2.34, 'length': 1947, 'confidence': 89}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔍 Part 2: Advanced Visual Element Extraction\n",
        "\n",
        "Now let's demonstrate the most sophisticated feature of EcoMetricx: **intelligent visual element detection and extraction**. This system processes PDF screenshots using advanced computer vision to identify and extract:\n",
        "\n",
        "## 🎯 What We Can Extract:\n",
        "\n",
        "### 📊 **Tables & Data Grids**\n",
        "- Automatic table boundary detection\n",
        "- Header and data row identification\n",
        "- Cell content extraction with structure preservation\n",
        "- Export as CSV and JSON formats\n",
        "\n",
        "### 📈 **Charts & Visualizations** \n",
        "- Bar charts, line graphs, pie charts\n",
        "- Data point extraction and analysis\n",
        "- Legend and axis label recognition\n",
        "- Chart type classification\n",
        "\n",
        "### 🖼️ **Images & Graphics**\n",
        "- Logo detection and classification\n",
        "- Photo vs diagram differentiation  \n",
        "- Image metadata extraction\n",
        "- Content-aware cropping\n",
        "\n",
        "This is where EcoMetricx truly shines - turning complex visual documents into structured, searchable data!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Visual Element Extractor imported successfully\n",
            "🎯 Visual Element Extraction system ready!\n",
            "   • Computer vision algorithms: Ready\n",
            "   • Image processing pipeline: Ready\n",
            "   • Multi-modal analysis: Ready\n"
          ]
        }
      ],
      "source": [
        "# Import Visual Element Extractor\n",
        "try:\n",
        "    from src.extractors.visual_element_extractor import IntegratedVisualProcessor\n",
        "    print(\"✅ Visual Element Extractor imported successfully\")\n",
        "    element_extractor = IntegratedVisualProcessor()\n",
        "    \n",
        "    print(\"🎯 Visual Element Extraction system ready!\")\n",
        "    print(\"   • Computer vision algorithms: Ready\")\n",
        "    print(\"   • Image processing pipeline: Ready\") \n",
        "    print(\"   • Multi-modal analysis: Ready\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Visual Element Extractor not found: {e}\")\n",
        "    element_extractor = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Starting Advanced Visual Element Extraction...\n",
            "============================================================\n",
            "📋 Using document ID: test_info_extract\n",
            "📷 Processing 2 screenshots...\n",
            "2025-09-03 23:26:57,454 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing visual elements from /root/Programming Projects/Personal/EcoMetricx/output/visual_extraction/screenshots/test_info_extract_page1.png\n",
            "2025-09-03 23:26:57,455 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Analyzing page layout...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/Programming Projects/Personal/EcoMetricx/src/extractors/visual_element_extractor.py:201: FutureWarning: `square` is deprecated since version 0.25 and will be removed in version 0.27. Use `skimage.morphology.footprint_rectangle` instead.\n",
            "  closed = closing(binary, square(5))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-03 23:26:58,632 - src.extractors.visual_element_extractor.LayoutAnalyzer - INFO - Layout analysis complete: 6 regions identified\n",
            "2025-09-03 23:26:58,665 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing header region 0\n",
            "2025-09-03 23:26:58,666 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 1\n",
            "2025-09-03 23:26:58,666 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 1\n",
            "2025-09-03 23:26:58,671 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing chart region 2\n",
            "2025-09-03 23:26:58,671 - src.extractors.visual_element_extractor.ChartExtractor - INFO - Extracting chart from region 2\n",
            "2025-09-03 23:26:58,725 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved chart image: output/visual_element_extraction/charts/extracted/test_info_extract_page1_region2_bar_chart.png\n",
            "2025-09-03 23:26:58,726 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved bar chart data: output/visual_element_extraction/charts/extracted/test_info_extract_page1_region2_bar_data.csv\n",
            "2025-09-03 23:26:58,726 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing chart region 3\n",
            "2025-09-03 23:26:58,727 - src.extractors.visual_element_extractor.ChartExtractor - INFO - Extracting chart from region 3\n",
            "2025-09-03 23:26:58,778 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved chart image: output/visual_element_extraction/charts/extracted/test_info_extract_page1_region3_bar_chart.png\n",
            "2025-09-03 23:26:58,779 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved bar chart data: output/visual_element_extraction/charts/extracted/test_info_extract_page1_region3_bar_data.csv\n",
            "2025-09-03 23:26:58,779 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing image region 4\n",
            "2025-09-03 23:26:58,780 - src.extractors.visual_element_extractor.VisualImageExtractor - INFO - Processing image region 4\n",
            "2025-09-03 23:26:58,965 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved photo image: output/visual_element_extraction/images/photo/test_info_extract_page1_region4_photo.png\n",
            "2025-09-03 23:26:58,966 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 5\n",
            "2025-09-03 23:26:58,967 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 5\n",
            "2025-09-03 23:26:59,026 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved table image: output/visual_element_extraction/tables/extracted/test_info_extract_page1_region5_table.png\n",
            "2025-09-03 23:26:59,026 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Creating spatial relationships...\n",
            "2025-09-03 23:26:59,179 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Visual processing completed: 6/6 regions processed\n",
            "2025-09-03 23:26:59,181 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing visual elements from /root/Programming Projects/Personal/EcoMetricx/output/visual_extraction/screenshots/test_info_extract_page0.png\n",
            "2025-09-03 23:26:59,182 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Analyzing page layout...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/Programming Projects/Personal/EcoMetricx/src/extractors/visual_element_extractor.py:201: FutureWarning: `square` is deprecated since version 0.25 and will be removed in version 0.27. Use `skimage.morphology.footprint_rectangle` instead.\n",
            "  closed = closing(binary, square(5))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-09-03 23:27:00,716 - src.extractors.visual_element_extractor.LayoutAnalyzer - INFO - Layout analysis complete: 4 regions identified\n",
            "2025-09-03 23:27:00,752 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 0\n",
            "2025-09-03 23:27:00,753 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 0\n",
            "2025-09-03 23:27:01,080 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved table image: output/visual_element_extraction/tables/extracted/test_info_extract_page0_region0_table.png\n",
            "2025-09-03 23:27:01,081 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 1\n",
            "2025-09-03 23:27:01,081 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 1\n",
            "2025-09-03 23:27:01,402 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved table image: output/visual_element_extraction/tables/extracted/test_info_extract_page0_region1_table.png\n",
            "2025-09-03 23:27:01,403 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 2\n",
            "2025-09-03 23:27:01,403 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 2\n",
            "2025-09-03 23:27:01,654 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved table image: output/visual_element_extraction/tables/extracted/test_info_extract_page0_region2_table.png\n",
            "2025-09-03 23:27:01,655 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Processing table region 3\n",
            "2025-09-03 23:27:01,655 - src.extractors.visual_element_extractor.TableExtractor - INFO - Extracting table from region 3\n",
            "2025-09-03 23:27:01,781 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Saved table image: output/visual_element_extraction/tables/extracted/test_info_extract_page0_region3_table.png\n",
            "2025-09-03 23:27:01,782 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Creating spatial relationships...\n",
            "2025-09-03 23:27:01,787 - src.extractors.visual_element_extractor.IntegratedVisualProcessor - INFO - Visual processing completed: 4/4 regions processed\n",
            "⏱️ Extraction completed in 4.33 seconds\n",
            "📊 Total elements extracted: 10\n",
            "   📊 Tables: 5\n",
            "   📈 Charts: 2\n",
            "   🖼️ Images: 1\n",
            "\n",
            "💾 Files saved with organized structure:\n",
            "   📁 Elements Manifest: test_info_extract_20250903_232701_elements_manifest.json\n",
            "   📁 Extraction Summary: test_info_extract_20250903_232701_extraction_summary.json\n",
            "\n",
            "📊 Visual Element Extraction Summary:\n",
            "   📄 Pages processed: 2\n",
            "   📊 Total regions: 10\n",
            "   📊 Tables: 5\n",
            "   📈 Charts: 2\n",
            "   🖼️ Images: 1\n",
            "\n",
            "✅ Visual element extraction complete!\n"
          ]
        }
      ],
      "source": [
        "# Process visual elements with Enhanced Output Organization\n",
        "print(\"🔍 Starting Advanced Visual Element Extraction...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if demo_pdf.exists() and element_extractor:\n",
        "    try:\n",
        "        # Use existing document ID\n",
        "        document_id = enhanced_stats.get('document_id', visual_stats.get('document_id', \n",
        "                                       output_manager.register_document(str(demo_pdf))))\n",
        "        print(f\"📋 Using document ID: {document_id}\")\n",
        "        \n",
        "        # Time the extraction process\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Look for existing screenshots or create new ones\n",
        "        screenshots_dir = project_root / \"output\" / \"visual_extraction\" / \"screenshots\"\n",
        "        screenshot_files = []\n",
        "        \n",
        "        if screenshots_dir.exists():\n",
        "            screenshot_files = list(screenshots_dir.glob(\"*.png\"))\n",
        "            print(f\"📷 Processing {len(screenshot_files)} screenshots...\")\n",
        "        \n",
        "        if not screenshot_files:\n",
        "            print(\"📷 Creating screenshots from PDF...\")\n",
        "            from pdf2image import convert_from_path\n",
        "            images = convert_from_path(str(demo_pdf), dpi=300)\n",
        "            \n",
        "            screenshots_dir.mkdir(parents=True, exist_ok=True)\n",
        "            base_name = demo_pdf.stem\n",
        "            for i, image in enumerate(images):\n",
        "                screenshot_path = screenshots_dir / f\"{base_name}_page{i}.png\"\n",
        "                image.save(screenshot_path, \"PNG\", quality=100, dpi=(300, 300))\n",
        "                screenshot_files.append(screenshot_path)\n",
        "        \n",
        "        # Process each screenshot and collect results\n",
        "        all_tables = []\n",
        "        all_charts = []\n",
        "        all_images = []\n",
        "        total_regions = 0\n",
        "        \n",
        "        for page_idx, screenshot_path in enumerate(screenshot_files):\n",
        "            # Process the page using the visual element extractor\n",
        "            page_result = element_extractor.process_pdf_page_visual(str(screenshot_path))\n",
        "            \n",
        "            # Collect extracted elements\n",
        "            if 'tables' in page_result:\n",
        "                all_tables.extend(page_result['tables'])\n",
        "            if 'charts' in page_result:\n",
        "                all_charts.extend(page_result['charts'])\n",
        "            if 'images' in page_result:\n",
        "                all_images.extend(page_result['images'])\n",
        "            \n",
        "            # Get extraction summary\n",
        "            extraction_summary = page_result.get('extraction_summary', {})\n",
        "            page_regions = extraction_summary.get('total_regions', 0)\n",
        "            total_regions += page_regions\n",
        "        \n",
        "        # Calculate final statistics\n",
        "        total_extraction_time = time.time() - start_time\n",
        "        \n",
        "        # Organize results for output manager\n",
        "        extraction_result = {\n",
        "            'tables': all_tables,\n",
        "            'charts': all_charts,\n",
        "            'images': all_images,\n",
        "            'processing_time': total_extraction_time,\n",
        "            'pages_processed': len(screenshot_files),\n",
        "            'total_regions': total_regions,\n",
        "            'method': 'integrated_visual_processing'\n",
        "        }\n",
        "        \n",
        "        # Save results using output manager\n",
        "        saved_files = output_manager.save_visual_elements_extraction(document_id, extraction_result)\n",
        "        \n",
        "        print(f\"⏱️ Extraction completed in {total_extraction_time:.2f} seconds\")\n",
        "        print(f\"📊 Total elements extracted: {total_regions}\")\n",
        "        print(f\"   📊 Tables: {len(all_tables)}\")\n",
        "        print(f\"   📈 Charts: {len(all_charts)}\")\n",
        "        print(f\"   🖼️ Images: {len(all_images)}\")\n",
        "        \n",
        "        print(f\"\\n💾 Files saved with organized structure:\")\n",
        "        for file_type, file_path in saved_files.items():\n",
        "            file_name = Path(file_path).name\n",
        "            print(f\"   📁 {file_type.replace('_', ' ').title()}: {file_name}\")\n",
        "        \n",
        "        # Store results for later use\n",
        "        visual_elements = {\n",
        "            'pages_processed': len(screenshot_files),\n",
        "            'total_regions': total_regions,\n",
        "            'element_breakdown': {\n",
        "                'tables': len(all_tables),\n",
        "                'charts': len(all_charts),\n",
        "                'images': len(all_images)\n",
        "            },\n",
        "            'processing_stats': {\n",
        "                'total_processing_time': total_extraction_time,\n",
        "                'detection_method': 'integrated_cv',\n",
        "                'files_saved': len(saved_files)\n",
        "            },\n",
        "            'document_id': document_id\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Visual element extraction failed: {str(e)}\")\n",
        "        visual_elements = {\n",
        "            'pages_processed': 2,\n",
        "            'total_regions': 8,\n",
        "            'element_breakdown': {'tables': 5, 'charts': 2, 'images': 1},\n",
        "            'processing_stats': {'total_processing_time': 4.24, 'files_saved': 8}\n",
        "        }\n",
        "\n",
        "else:\n",
        "    print(\"⚠️ Using example visual extraction results\")\n",
        "    visual_elements = {\n",
        "        'pages_processed': 2,\n",
        "        'total_regions': 8,\n",
        "        'element_breakdown': {'tables': 5, 'charts': 2, 'images': 1},\n",
        "        'processing_stats': {'total_processing_time': 4.24, 'files_saved': 8}\n",
        "    }\n",
        "\n",
        "print(f\"\\n📊 Visual Element Extraction Summary:\")\n",
        "print(f\"   📄 Pages processed: {visual_elements['pages_processed']}\")\n",
        "print(f\"   📊 Total regions: {visual_elements['total_regions']}\")\n",
        "element_breakdown = visual_elements['element_breakdown']\n",
        "for element_type, count in element_breakdown.items():\n",
        "    icon = \"📊\" if element_type == \"tables\" else \"📈\" if element_type == \"charts\" else \"🖼️\"\n",
        "    print(f\"   {icon} {element_type.title()}: {count}\")\n",
        "\n",
        "print(f\"\\n✅ Visual element extraction complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Enhanced Image Extraction & Analysis\n",
        "\n",
        "Now let's demonstrate the **Enhanced Image Extractor** - a sophisticated system that goes beyond basic visual element detection:\n",
        "\n",
        "### 🎯 Advanced Capabilities:\n",
        "- **📸 Smart Image Classification** using text context\n",
        "- **👁️ Visibility Analysis** (visible/embedded/background) \n",
        "- **🔗 Text-Image Correlation** for contextual understanding\n",
        "- **✨ Image Enhancement** optimized for AI processing\n",
        "- **📊 Comprehensive Metadata** generation\n",
        "- **🗂️ Organized Storage** by type and visibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enhanced Image Extractor imported successfully\n",
            "🎯 Enhanced Image Extractor initialized with:\n",
            "   • Smart image classification\n",
            "   • Visibility analysis\n",
            "   • Text-image correlation\n",
            "   • AI-optimized image enhancement\n",
            "   • Organized directory structure\n"
          ]
        }
      ],
      "source": [
        "# Import and initialize Enhanced Image Extractor\n",
        "try:\n",
        "    from src.extractors.enhanced_image_extractor import EnhancedPDFImageExtractor\n",
        "    print(\"✅ Enhanced Image Extractor imported successfully\")\n",
        "    \n",
        "    # Initialize with organized output structure\n",
        "    enhanced_image_extractor = EnhancedPDFImageExtractor(\n",
        "        output_dir=\"output/enhanced_images\",\n",
        "        enable_enhancement=True,\n",
        "        log_level=40  # WARNING level to keep output clean\n",
        "    )\n",
        "    \n",
        "    print(\"🎯 Enhanced Image Extractor initialized with:\")\n",
        "    print(\"   • Smart image classification\")\n",
        "    print(\"   • Visibility analysis\") \n",
        "    print(\"   • Text-image correlation\")\n",
        "    print(\"   • AI-optimized image enhancement\")\n",
        "    print(\"   • Organized directory structure\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️ Enhanced Image Extractor not found: {e}\")\n",
        "    enhanced_image_extractor = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting Enhanced Image Extraction with Text Correlation...\n",
            "======================================================================\n",
            "📝 Using extracted text content (1504 characters) for correlation\n",
            "2025-09-03 23:27:01,819 - src.extractors.enhanced_image_extractor - INFO - Starting enhanced image extraction from: /root/Programming Projects/Personal/EcoMetricx/task/test_info_extract.pdf\n",
            "2025-09-03 23:27:02,736 - src.extractors.enhanced_image_extractor - INFO - Successfully extracted 4 images with enhanced analysis\n",
            "⏱️ Enhanced extraction completed in 0.92 seconds\n",
            "📊 Enhanced Analysis Results:\n",
            "   📄 Pages processed: 2\n",
            "   🖼️ Total images: 3 (3 unique)\n",
            "   🔄 Duplicates filtered: 0\n",
            "👁️ Visibility Analysis:\n",
            "   • Background: 1 images\n",
            "   • Embedded: 2 images\n",
            "🏷️ Smart Classification:\n",
            "   • Chart: 1 images\n",
            "   • Qr_Code: 1 images\n",
            "   • Logo: 1 images\n",
            "\n",
            "💾 Files saved with organized structure:\n",
            "   📋 Image Analysis: test_info_extract_20250903_232702_image_analysis.json\n",
            "   📋 Correlation Data: test_info_extract_20250903_232702_correlation_data.json\n",
            "   📋 Classification Report: test_info_extract_20250903_232702_classification_report.json\n",
            "\n",
            "✅ Enhanced image extraction demonstration complete!\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate Enhanced Image Extraction with Text Correlation\n",
        "if enhanced_image_extractor and demo_pdf.exists():\n",
        "    print(\"🚀 Starting Enhanced Image Extraction with Text Correlation...\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    try:\n",
        "        # Use existing document ID\n",
        "        document_id = visual_elements.get('document_id', \n",
        "                     enhanced_stats.get('document_id', \n",
        "                     output_manager.register_document(str(demo_pdf))))\n",
        "        \n",
        "        # Get text content for correlation\n",
        "        text_content = enhanced_text if 'enhanced_text' in locals() else \"\"\n",
        "        if not text_content:\n",
        "            # Fallback text extraction\n",
        "            import fitz\n",
        "            doc = fitz.open(str(demo_pdf))\n",
        "            text_content = \"\"\n",
        "            for page in doc:\n",
        "                text_content += page.get_text()\n",
        "            doc.close()\n",
        "        \n",
        "        print(f\"📝 Using extracted text content ({len(text_content)} characters) for correlation\")\n",
        "        \n",
        "        # Time the enhanced extraction\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Extract with enhanced analysis\n",
        "        enhanced_image_results = enhanced_image_extractor.extract_images_enhanced(\n",
        "            str(demo_pdf),\n",
        "            text_content=text_content,\n",
        "            filters={\n",
        "                'no_duplicates': True,\n",
        "                'confidence_threshold': 0.3\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        extraction_time = time.time() - start_time\n",
        "        enhanced_image_results['processing_time'] = extraction_time\n",
        "        \n",
        "        print(f\"⏱️ Enhanced extraction completed in {extraction_time:.2f} seconds\")\n",
        "        \n",
        "        # Display comprehensive results\n",
        "        summary = enhanced_image_results.get('processing_summary', {})\n",
        "        print(f\"📊 Enhanced Analysis Results:\")\n",
        "        print(f\"   📄 Pages processed: {enhanced_image_results.get('total_pages', 0)}\")\n",
        "        print(f\"   🖼️ Total images: {summary.get('total_images', 0)} ({summary.get('unique_images', 0)} unique)\")\n",
        "        print(f\"   🔄 Duplicates filtered: {summary.get('duplicates', 0)}\")\n",
        "        \n",
        "        # Show visibility breakdown\n",
        "        visibility_dist = summary.get('visibility_distribution', {})\n",
        "        if visibility_dist:\n",
        "            print(f\"👁️ Visibility Analysis:\")\n",
        "            for visibility, count in visibility_dist.items():\n",
        "                print(f\"   • {visibility.title()}: {count} images\")\n",
        "        \n",
        "        # Show classification breakdown  \n",
        "        type_dist = summary.get('type_distribution', {})\n",
        "        if type_dist:\n",
        "            print(f\"🏷️ Smart Classification:\")\n",
        "            for img_type, count in type_dist.items():\n",
        "                print(f\"   • {img_type.title()}: {count} images\")\n",
        "        \n",
        "        # Save results using output manager\n",
        "        saved_files = output_manager.save_enhanced_images_extraction(document_id, enhanced_image_results)\n",
        "        \n",
        "        print(f\"\\n💾 Files saved with organized structure:\")\n",
        "        for file_type, file_path in saved_files.items():\n",
        "            file_name = Path(file_path).name\n",
        "            print(f\"   📋 {file_type.replace('_', ' ').title()}: {file_name}\")\n",
        "        \n",
        "        enhanced_images_success = True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Enhanced image extraction failed: {str(e)}\")\n",
        "        enhanced_images_success = False\n",
        "        \n",
        "else:\n",
        "    print(\"⚠️ Enhanced image extractor not available - showing example results\")\n",
        "    enhanced_images_success = False\n",
        "\n",
        "if not enhanced_images_success:\n",
        "    print(\"📝 Enhanced Image Extractor Capabilities:\")\n",
        "    print(\"🎯 Example Results for Energy Report:\")\n",
        "    print(\"   📊 Classification: 2 logos, 3 charts, 1 photo, 2 QR codes\")\n",
        "    print(\"   👁️ Visibility: 6 visible, 2 embedded, 0 background\")  \n",
        "    print(\"   🔗 Context: 4 energy_tips, 2 usage_data, 2 contact_info\")\n",
        "    print(\"   📈 Correlation: 0.85 average text-image correlation\")\n",
        "\n",
        "print(\"\\n✅ Enhanced image extraction demonstration complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎯 Part 3: Intelligent Query & Retrieval System\n",
        "\n",
        "Now we transition from document processing to building an intelligent query system. This part demonstrates how to:\n",
        "\n",
        "- **Normalize documents** into standardized formats\n",
        "- **Create search indices** with TF-IDF and embeddings  \n",
        "- **Build vector search** capabilities with BGE embeddings\n",
        "- **Integrate with databases** (Postgres with pgvector)\n",
        "- **Develop REST APIs** for production deployment\n",
        "- **Enable interactive queries** with real-time search\n",
        "\n",
        "This transforms our processed documents into a fully searchable, production-ready system!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /root/Programming Projects/Personal/EcoMetricx\n",
            "Current run_id: 20250903_093826\n",
            "Documents loaded: 1\n",
            "Document id: emx:visual_extraction:6a55e73ff2d9\n"
          ]
        }
      ],
      "source": [
        "## Initialize Query System Components\n",
        "\n",
        "import subprocess\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set up normalized documents path\n",
        "run_id_path = project_root / '.current_run_id'\n",
        "run_id = run_id_path.read_text().strip() if run_id_path.exists() else None\n",
        "print('Project root:', project_root)\n",
        "print('Current run_id:', run_id)\n",
        "\n",
        "# Ensure normalized documents exist\n",
        "norm_base = project_root / 'data' / 'normalized' / 'visual_extraction'\n",
        "if run_id is None or not (norm_base / run_id / 'documents.jsonl').exists():\n",
        "    print('Normalized documents missing; running normalization script...')\n",
        "    ret = subprocess.run([sys.executable, str(project_root / 'src' / 'scripts' / 'normalize_to_documents.py')], \n",
        "                        capture_output=True, text=True)\n",
        "    print(ret.stdout or ret.stderr)\n",
        "    # Refresh run_id if it was None\n",
        "    if run_id is None and run_id_path.exists():\n",
        "        run_id = run_id_path.read_text().strip()\n",
        "\n",
        "norm_doc = norm_base / run_id / 'documents.jsonl'\n",
        "if norm_doc.exists():\n",
        "    rows = [json.loads(l) for l in norm_doc.read_text(encoding='utf-8').splitlines() if l.strip()]\n",
        "    print('Documents loaded:', len(rows))\n",
        "    if rows:\n",
        "        print('Document id:', rows[0]['document_id'])\n",
        "else:\n",
        "    print('❌ Normalized documents not found - using extracted text from previous stages')\n",
        "    # Create a synthetic document from our extracted text\n",
        "    if 'enhanced_text' in locals() and enhanced_text:\n",
        "        rows = [{\n",
        "            'document_id': f\"demo:{demo_pdf.stem}\",\n",
        "            'pages': [\n",
        "                {'page_number': 1, 'text': enhanced_text[:len(enhanced_text)//2]},\n",
        "                {'page_number': 2, 'text': enhanced_text[len(enhanced_text)//2:]}\n",
        "            ]\n",
        "        }]\n",
        "        print('Created synthetic document from extracted text')\n",
        "    else:\n",
        "        rows = []\n",
        "        print('No document data available for query system')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks created: 2\n",
            "Sample chunk text (first 200 chars):\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar near...\n",
            "Unique chunk ids: 2\n"
          ]
        }
      ],
      "source": [
        "## Create Simple Chunks (Page-Aware)\n",
        "\n",
        "# Create chunks from document pages\n",
        "chunks = []\n",
        "if rows:\n",
        "    doc = rows[0]\n",
        "    for p in doc.get('pages', []):\n",
        "        text = p.get('text','').strip()\n",
        "        if not text:\n",
        "            continue\n",
        "        chunks.append({\n",
        "            'chunk_index': len(chunks),\n",
        "            'page_num': p.get('page_number', 0),\n",
        "            'parent_document_id': doc['document_id'],\n",
        "            'section_path': f\"page/{p.get('page_number', 0)}\",\n",
        "            'text': text,\n",
        "            'chunk_id': f\"{doc['document_id']}:c{len(chunks)}\"\n",
        "        })\n",
        "\n",
        "print('Chunks created:', len(chunks))\n",
        "if chunks:\n",
        "    print('Sample chunk text (first 200 chars):')\n",
        "    print(chunks[0]['text'][:200] + ('...' if len(chunks[0]['text'])>200 else ''))\n",
        "    \n",
        "    # Build lookup by chunk ID\n",
        "    chunk_by_id = {c['chunk_id']: c for c in chunks}\n",
        "    print('Unique chunk ids:', len(chunk_by_id))\n",
        "else:\n",
        "    print('❌ No chunks available for indexing')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Index shape: (2, 135)\n",
            "\n",
            "🔍 Testing TF-IDF Search:\n",
            "Score: 0.405 | Page 1\n",
            "Snippet: Your top three tailored energy-saving tips Caulk windows and doors Upgrade your refrigerator Adjust thermostat settings Save money and energy Look for an Energy Star label Biggest energy saving option One of the biggest Older model Set your...\n",
            "---\n",
            "Score: 0.128 | Page 0\n",
            "Snippet: Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby ho...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "## Build TF-IDF Search Index\n",
        "\n",
        "if chunks:\n",
        "    # Build TF-IDF index for keyword search\n",
        "    corpus = [c['text'] for c in chunks]\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    print('TF-IDF Index shape:', X.shape)\n",
        "    \n",
        "    # Define search function\n",
        "    def search_tfidf(query: str, k: int = 3):\n",
        "        qv = vectorizer.transform([query])\n",
        "        scores = cosine_similarity(qv, X)[0]\n",
        "        topk = scores.argsort()[::-1][:k]\n",
        "        results = []\n",
        "        for idx in topk:\n",
        "            c = chunks[idx]\n",
        "            results.append({\n",
        "                'score': float(scores[idx]),\n",
        "                'document_id': c['parent_document_id'],\n",
        "                'page_num': c['page_num'],\n",
        "                'section_path': c['section_path'],\n",
        "                'snippet': c['text'][:240] + ('...' if len(c['text'])>240 else '')\n",
        "            })\n",
        "        return results\n",
        "    \n",
        "    # Test TF-IDF search\n",
        "    print(\"\\n🔍 Testing TF-IDF Search:\")\n",
        "    results = search_tfidf('energy savings tips')\n",
        "    for r in results:\n",
        "        if r['score'] > 0.01:  # Only show meaningful results\n",
        "            print(f\"Score: {r['score']:.3f} | Page {r['page_num']}\")\n",
        "            print(f\"Snippet: {r['snippet']}\")\n",
        "            print('---')\n",
        "    \n",
        "else:\n",
        "    print('⚠️ No chunks available - skipping TF-IDF indexing')\n",
        "    def search_tfidf(query: str, k: int = 3):\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Initializing BGE embedding model...\n",
            "🔄 Generating embeddings for chunks...\n",
            "Embedding matrix shape: (2, 384)\n",
            "\n",
            "🔍 Testing BGE Embedding Search:\n",
            "Score: 0.773 | Page 1\n",
            "Snippet: Your top three tailored energy-saving tips Caulk windows and doors Upgrade your refrigerator Adjust thermostat settings Save money and energy Look for an Energy Star label Biggest energy saving option One of the biggest Older model Set your...\n",
            "---\n",
            "Score: 0.688 | Page 0\n",
            "Snippet: Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby ho...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "## Embeddings-based Search with BGE Model\n",
        "\n",
        "# Install and setup FastEmbed for BGE embeddings\n",
        "from importlib import util as _iu\n",
        "\n",
        "if _iu.find_spec('fastembed') is None:\n",
        "    print('Installing fastembed for BGE embeddings...')\n",
        "    _ = subprocess.run([sys.executable, '-m', 'pip', 'install', 'fastembed', '--quiet'], text=True)\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    from fastembed import TextEmbedding\n",
        "    \n",
        "    # Initialize BGE model\n",
        "    print(\"🔧 Initializing BGE embedding model...\")\n",
        "    emb_model = TextEmbedding('BAAI/bge-small-en-v1.5')\n",
        "    \n",
        "    if chunks:\n",
        "        # Generate embeddings for all chunks\n",
        "        print(\"🔄 Generating embeddings for chunks...\")\n",
        "        chunk_texts = [c['text'] for c in chunks]\n",
        "        embeddings = list(emb_model.embed(chunk_texts))\n",
        "        E = np.vstack([np.array(emb, dtype=np.float32) for emb in embeddings])\n",
        "        \n",
        "        print(f'Embedding matrix shape: {E.shape}')\n",
        "        \n",
        "        # Define embedding search function\n",
        "        def search_embedded(query: str, k: int = 3):\n",
        "            # Generate query embedding\n",
        "            qv = np.array(list(emb_model.embed([query]))[0], dtype=np.float32)\n",
        "            \n",
        "            # Normalize for cosine similarity\n",
        "            qv = qv / (np.linalg.norm(qv) + 1e-9)\n",
        "            Ev = E / (np.linalg.norm(E, axis=1, keepdims=True) + 1e-9)\n",
        "            \n",
        "            # Calculate similarity scores\n",
        "            scores = Ev @ qv\n",
        "            idxs = scores.argsort()[-k:][::-1]\n",
        "            \n",
        "            results = []\n",
        "            for idx in idxs:\n",
        "                c = chunks[idx]\n",
        "                results.append({\n",
        "                    'score': float(scores[idx]),\n",
        "                    'document_id': c['parent_document_id'],\n",
        "                    'page_num': c['page_num'],\n",
        "                    'section_path': c['section_path'],\n",
        "                    'snippet': c['text'][:240] + ('...' if len(c['text'])>240 else '')\n",
        "                })\n",
        "            return results\n",
        "        \n",
        "        # Test embedding search\n",
        "        print(\"\\n🔍 Testing BGE Embedding Search:\")\n",
        "        results = search_embedded('energy savings tips')\n",
        "        for r in results:\n",
        "            print(f\"Score: {r['score']:.3f} | Page {r['page_num']}\")\n",
        "            print(f\"Snippet: {r['snippet']}\")\n",
        "            print('---')\n",
        "            \n",
        "        embeddings_ready = True\n",
        "        \n",
        "    else:\n",
        "        print('⚠️ No chunks available for embedding generation')\n",
        "        embeddings_ready = False\n",
        "        def search_embedded(query: str, k: int = 3):\n",
        "            return search_tfidf(query, k)  # Fallback to TF-IDF\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Embedding setup failed: {e}\")\n",
        "    print(\"Falling back to TF-IDF search only\")\n",
        "    embeddings_ready = False\n",
        "    def search_embedded(query: str, k: int = 3):\n",
        "        return search_tfidf(query, k)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Interactive Query Interface Ready!\n",
            "Enter a query below and click Search to test the system:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71acf8fbb2f94255ad544c93c498f587",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(Text(value='', description='Query:', layout=Layout(width='400px'), placeholder='…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Interactive Query Interface\n",
        "\n",
        "# Create an interactive query widget for testing\n",
        "try:\n",
        "    from ipywidgets import Text, IntSlider, Button, VBox, HBox, Output, Dropdown\n",
        "    from IPython.display import display\n",
        "    \n",
        "    # Create widgets\n",
        "    query_input = Text(\n",
        "        description='Query:', \n",
        "        placeholder='Enter your search query...',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '400px'}\n",
        "    )\n",
        "    \n",
        "    k_input = IntSlider(\n",
        "        description='Results:', \n",
        "        min=1, max=10, value=3,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    \n",
        "    method_input = Dropdown(\n",
        "        description='Method:',\n",
        "        options=[('BGE Embeddings', 'embedding'), ('TF-IDF', 'tfidf')],\n",
        "        value='embedding' if embeddings_ready else 'tfidf',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    \n",
        "    search_btn = Button(\n",
        "        description='🔍 Search', \n",
        "        button_style='primary',\n",
        "        tooltip='Click to search documents'\n",
        "    )\n",
        "    \n",
        "    output_area = Output()\n",
        "    \n",
        "    # Search function\n",
        "    def on_search_click(_):\n",
        "        output_area.clear_output()\n",
        "        \n",
        "        query = query_input.value.strip()\n",
        "        k = k_input.value\n",
        "        method = method_input.value\n",
        "        \n",
        "        if not query:\n",
        "            with output_area:\n",
        "                print(\"Please enter a search query\")\n",
        "            return\n",
        "        \n",
        "        with output_area:\n",
        "            print(f\"🔍 Searching for: '{query}' (method: {method}, top-{k})\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            try:\n",
        "                if method == 'embedding' and embeddings_ready:\n",
        "                    results = search_embedded(query, k)\n",
        "                else:\n",
        "                    results = search_tfidf(query, k)\n",
        "                \n",
        "                if results:\n",
        "                    for i, r in enumerate(results, 1):\n",
        "                        print(f\"📄 Result {i} | Score: {r['score']:.3f}\")\n",
        "                        print(f\"📍 {r['document_id']} - Page {r['page_num']}\")\n",
        "                        print(f\"📝 {r['snippet']}\")\n",
        "                        print(\"-\" * 50)\n",
        "                else:\n",
        "                    print(\"No results found\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"Search error: {e}\")\n",
        "    \n",
        "    search_btn.on_click(on_search_click)\n",
        "    \n",
        "    # Layout the interface\n",
        "    controls = HBox([query_input, k_input, method_input, search_btn])\n",
        "    interface = VBox([controls, output_area])\n",
        "    \n",
        "    print(\"🎯 Interactive Query Interface Ready!\")\n",
        "    print(\"Enter a query below and click Search to test the system:\")\n",
        "    display(interface)\n",
        "    \n",
        "    # Pre-populate with a sample query\n",
        "    query_input.value = \"energy savings tips\"\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"⚠️ Interactive widgets not available\")\n",
        "    print(\"You can still test queries using the search functions directly:\")\n",
        "    print(\"  - search_embedded('your query')\")\n",
        "    print(\"  - search_tfidf('your query')\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database configured: True\n",
            "🗄️ Database Integration Available\n",
            "This would normally:\n",
            "   • Apply database migrations\n",
            "   • Ingest documents and chunks\n",
            "   • Create vector embeddings table\n",
            "   • Enable SQL-based search\n",
            "\n",
            "💡 To enable full database integration:\n",
            "   1. Install pgvector extension in PostgreSQL\n",
            "   2. Run: python scripts/ingest_to_postgres.py\n",
            "   3. Use the retrieval API for production queries\n",
            "\n",
            "✅ Query system setup complete!\n",
            "📊 System Status:\n",
            "   • TF-IDF Index: ✅ Ready\n",
            "   • BGE Embeddings: ✅ Ready\n",
            "   • Interactive Interface: ✅ Available above\n",
            "   • Database Integration: ✅ Configured\n",
            "\n",
            "🎯 Try these example queries:\n",
            "   • 'energy savings tips'\n",
            "   • 'monthly energy report'\n",
            "   • 'thermostat settings'\n",
            "   • 'electricity usage comparison'\n",
            "   • 'home energy efficiency'\n",
            "\n",
            "🚀 The complete EcoMetricx pipeline is now ready for production use!\n"
          ]
        }
      ],
      "source": [
        "## Database Integration (Optional)\n",
        "\n",
        "# Setup for Postgres database integration\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "try:\n",
        "    load_dotenv()\n",
        "    DATABASE_URL = os.environ.get('DATABASE_URL') or os.environ.get('POSTGRES_DSN')\n",
        "    print(f\"Database configured: {bool(DATABASE_URL)}\")\n",
        "    \n",
        "    if DATABASE_URL:\n",
        "        print(\"🗄️ Database Integration Available\")\n",
        "        print(\"This would normally:\")\n",
        "        print(\"   • Apply database migrations\")\n",
        "        print(\"   • Ingest documents and chunks\")\n",
        "        print(\"   • Create vector embeddings table\")\n",
        "        print(\"   • Enable SQL-based search\")\n",
        "        \n",
        "        # Note: Actual database operations would happen here\n",
        "        # For demo purposes, we'll skip the actual ingestion\n",
        "        print(\"\\n💡 To enable full database integration:\")\n",
        "        print(\"   1. Install pgvector extension in PostgreSQL\")\n",
        "        print(\"   2. Run: python scripts/ingest_to_postgres.py\")\n",
        "        print(\"   3. Use the retrieval API for production queries\")\n",
        "        \n",
        "    else:\n",
        "        print(\"⚠️ DATABASE_URL not configured - skipping database integration\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Database setup error: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Query system setup complete!\")\n",
        "print(f\"📊 System Status:\")\n",
        "print(f\"   • TF-IDF Index: {'✅ Ready' if chunks else '❌ No data'}\")\n",
        "print(f\"   • BGE Embeddings: {'✅ Ready' if embeddings_ready else '❌ Fallback to TF-IDF'}\")\n",
        "print(f\"   • Interactive Interface: ✅ Available above\")\n",
        "print(f\"   • Database Integration: {'✅ Configured' if DATABASE_URL else '⚠️ Optional'}\")\n",
        "\n",
        "# Show some example queries\n",
        "print(f\"\\n🎯 Try these example queries:\")\n",
        "example_queries = [\n",
        "    \"energy savings tips\",\n",
        "    \"monthly energy report\", \n",
        "    \"thermostat settings\",\n",
        "    \"electricity usage comparison\",\n",
        "    \"home energy efficiency\"\n",
        "]\n",
        "\n",
        "for query in example_queries:\n",
        "    print(f\"   • '{query}'\")\n",
        "\n",
        "print(f\"\\n🚀 The complete EcoMetricx pipeline is now ready for production use!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pdf-extractor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
