{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EcoMetricx â€” Retrieval Demo (MVP)\n",
        "\n",
        "This notebook demos the early query system pipeline:\n",
        "- Load normalized documents (Phase 2 output)\n",
        "- If missing, auto-run normalization to generate it\n",
        "- Chunk into retrieval units (simple, page-aware)\n",
        "- Build a TF-IDF index (CPU-only)\n",
        "- Run keyword/semantic-ish search and show citations\n",
        "\n",
        "Run cells top-to-bottom to try queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /root/Programming Projects/Personal/EcoMetricx\n",
            "Current run_id: 20250903_093826\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json, subprocess, sys\n",
        "from datetime import datetime\n",
        "\n",
        "project_root = Path.cwd()\n",
        "run_id_path = project_root / '.current_run_id'\n",
        "run_id = run_id_path.read_text().strip() if run_id_path.exists() else None\n",
        "print('Project root:', project_root)\n",
        "print('Current run_id:', run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents loaded: 1\n",
            "Document id: emx:visual_extraction:6a55e73ff2d9\n"
          ]
        }
      ],
      "source": [
        "# Ensure normalized documents exist; auto-run normalization if missing\n",
        "norm_base = project_root / 'data' / 'normalized' / 'visual_extraction'\n",
        "if run_id is None or not (norm_base / run_id / 'documents.jsonl').exists():\n",
        "\tprint('Normalized documents missing; running normalization script...')\n",
        "\tret = subprocess.run([sys.executable, str(project_root / 'scripts' / 'normalize_to_documents.py')], capture_output=True, text=True)\n",
        "\tprint(ret.stdout or ret.stderr)\n",
        "\t# Refresh run_id if it was None\n",
        "\tif run_id is None and run_id_path.exists():\n",
        "\t\trun_id = run_id_path.read_text().strip()\n",
        "\n",
        "norm_doc = norm_base / run_id / 'documents.jsonl'\n",
        "assert norm_doc.exists(), f'Missing {norm_doc}'\n",
        "rows = [json.loads(l) for l in norm_doc.read_text(encoding='utf-8').splitlines() if l.strip()]\n",
        "print('Documents loaded:', len(rows))\n",
        "print('Document id:', rows[0]['document_id'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create simple chunks (page-aware)\n",
        "We split the document text by pages into retrieval units and attach page numbers and a section path placeholder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks loaded: 1\n",
            "Sample chunk: {'parent_document_id': 'emx:visual_extraction:6a55e73ff2d9', 'page_num': 1, 'section_path': 'page/1'}\n"
          ]
        }
      ],
      "source": [
        "chunk_file = project_root / 'data' / 'chunks' / 'visual_extraction' / run_id / 'chunks.jsonl'\n",
        "if not chunk_file.exists():\n",
        "\tprint('Chunks missing; running chunking script...')\n",
        "\tret = subprocess.run([sys.executable, str(project_root / 'scripts' / 'chunk_and_redact.py')], capture_output=True, text=True)\n",
        "\tprint(ret.stdout or ret.stderr)\n",
        "\n",
        "chunks = [json.loads(l) for l in chunk_file.read_text(encoding='utf-8').splitlines() if l.strip()]\n",
        "print('Chunks loaded:', len(chunks))\n",
        "print('Sample chunk:', {k: chunks[0][k] for k in ('parent_document_id','page_num','section_path')})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunks: 2\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar near...\n"
          ]
        }
      ],
      "source": [
        "doc = rows[0]\n",
        "chunks = []\n",
        "for p in doc.get('pages', []):\n",
        "\ttext = p.get('text','').strip()\n",
        "\tif not text:\n",
        "\t\tcontinue\n",
        "\tchunks.append({\n",
        "\t\t'chunk_index': len(chunks),\n",
        "\t\t'page_num': p.get('page_number', 0),\n",
        "\t\t'parent_document_id': doc['document_id'],\n",
        "\t\t'section_path': f\"page/{p.get('page_number', 0)}\",\n",
        "\t\t'text': text\n",
        "\t})\n",
        "print('Chunks:', len(chunks))\n",
        "print(chunks[0]['text'][:200] + ('...' if len(chunks[0]['text'])>200 else ''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build TF-IDF index\n",
        "We use scikit-learn's `TfidfVectorizer` to index chunk texts for quick keyword/semantic-ish search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index shape: (2, 135)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "corpus = [c['text'] for c in chunks]\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print('Index shape:', X.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try a query\n",
        "Enter a query. We compute cosine similarity in TF-IDF space and show top results with citations (document id + page).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings-based retrieval (BGE-small)\n",
        "We use FastEmbed with `BAAI/bge-small-en-v1.5` to embed chunks and queries, then perform cosine similarity search. Falls back to TF-IDF if unavailable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing fastembed in current kernel...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings missing; generating...\n",
            "Wrote data/index/pgvector/20250903_093826/embeddings.jsonl and data/index/pgvector/20250903_093826/index_manifest.json\n",
            "\n",
            "Embeddings ready: /root/Programming Projects/Personal/EcoMetricx/data/index/pgvector/20250903_093826/embeddings.jsonl\n",
            "Model: BAAI/bge-small-en-v1.5 dim: 384\n"
          ]
        }
      ],
      "source": [
        "# Ensure embeddings exist; auto-generate if missing (auto-install fastembed)\n",
        "from importlib import util as _iu\n",
        "\n",
        "emb_dir = project_root / 'data' / 'index' / 'pgvector' / run_id\n",
        "emb_file = emb_dir / 'embeddings.jsonl'\n",
        "manifest_file = emb_dir / 'index_manifest.json'\n",
        "\n",
        "# Ensure fastembed is available in this kernel\n",
        "if _iu.find_spec('fastembed') is None:\n",
        "\tprint('Installing fastembed in current kernel...')\n",
        "\t_ = subprocess.run([sys.executable, '-m', 'pip', 'install', 'fastembed', '--quiet'], text=True)\n",
        "\n",
        "# Generate embeddings if missing\n",
        "if not emb_file.exists():\n",
        "\tprint('Embeddings missing; generating...')\n",
        "\tret = subprocess.run([sys.executable, str(project_root / 'scripts' / 'embed_chunks.py')], capture_output=True, text=True)\n",
        "\tprint(ret.stdout or ret.stderr)\n",
        "\n",
        "if emb_file.exists():\n",
        "\tprint('Embeddings ready:', emb_file)\n",
        "\tmanifest = json.loads(manifest_file.read_text())\n",
        "\tprint('Model:', manifest.get('model'), 'dim:', manifest.get('embedding_dim'))\n",
        "else:\n",
        "\tprint('Embeddings not available; search will use TF-IDF fallback.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix: (1, 384)\n"
          ]
        }
      ],
      "source": [
        "# Build in-memory embedding matrix if available\n",
        "import numpy as np\n",
        "\n",
        "emb_records = []\n",
        "if emb_file.exists():\n",
        "\tfor line in emb_file.read_text().splitlines():\n",
        "\t\tif not line.strip():\n",
        "\t\t\tcontinue\n",
        "\t\tr = json.loads(line)\n",
        "\t\temb_records.append(r)\n",
        "\tE = np.vstack([np.array(r['embedding_vector'], dtype=np.float32) for r in emb_records]) if emb_records else None\n",
        "\tid_to_idx = {r['chunk_id']: i for i, r in enumerate(emb_records)}\n",
        "\tprint('Embedding matrix:', E.shape if E is not None else None)\n",
        "else:\n",
        "\tE = None\n",
        "\tid_to_idx = {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalize chunk IDs (safety)\n",
        "Ensure every chunk has a `chunk_id` and build a lookup by id. This prevents KeyError if earlier cells created temporary chunks without IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique chunk ids: 2\n"
          ]
        }
      ],
      "source": [
        "# Ensure each chunk has a stable chunk_id and build lookup\n",
        "for i, c in enumerate(chunks):\n",
        "\tif 'chunk_id' not in c:\n",
        "\t\tc['chunk_id'] = f\"{c['parent_document_id']}:c{c.get('chunk_index', i)}\"\n",
        "chunk_by_id = {c['chunk_id']: c for c in chunks}\n",
        "print('Unique chunk ids:', len(chunk_by_id))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7733845710754395 emx:visual_extraction:6a55e73ff2d9 page 0\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby ho...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Override search_embedded to use chunk_by_id\n",
        "import numpy as np\n",
        "\n",
        "def search_embedded(query: str, k: int = 3):\n",
        "\tif E is None:\n",
        "\t\tprint('Embeddings not loaded; falling back to TF-IDF search()')\n",
        "\t\treturn search(query, k)\n",
        "\tfrom fastembed import TextEmbedding\n",
        "\temb = TextEmbedding('BAAI/bge-small-en-v1.5')\n",
        "\tqv = np.array(list(emb.embed([query]))[0], dtype=np.float32)\n",
        "\tqv = qv / (np.linalg.norm(qv) + 1e-9)\n",
        "\tEv = E / (np.linalg.norm(E, axis=1, keepdims=True) + 1e-9)\n",
        "\tscores = Ev @ qv\n",
        "\tidxs = scores.argsort()[-k:][::-1]\n",
        "\tresults = []\n",
        "\tfor idx in idxs:\n",
        "\t\tchunk_id = emb_records[idx]['chunk_id']\n",
        "\t\tc = chunk_by_id.get(chunk_id)\n",
        "\t\tif not c:\n",
        "\t\t\tcontinue\n",
        "\t\tresults.append({\n",
        "\t\t\t'score': float(scores[idx]),\n",
        "\t\t\t'document_id': c['parent_document_id'],\n",
        "\t\t\t'page_num': c['page_num'],\n",
        "\t\t\t'section_path': c['section_path'],\n",
        "\t\t\t'snippet': c['text'][:240] + ('...' if len(c['text'])>240 else '')\n",
        "\t\t})\n",
        "\treturn results\n",
        "\n",
        "# Demo\n",
        "for r in search_embedded('energy savings tips'):\n",
        "\tprint(r['score'], r['document_id'], f\"page {r['page_num']}\")\n",
        "\tprint(r['snippet'])\n",
        "\tprint('---')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique chunk ids: 2\n"
          ]
        }
      ],
      "source": [
        "# Backfill chunk_id if missing and build map\n",
        "for i, c in enumerate(chunks):\n",
        "\tif 'chunk_id' not in c:\n",
        "\t\tc['chunk_id'] = f\"{c['parent_document_id']}:c{c.get('chunk_index', i)}\"\n",
        "chunk_by_id = {c['chunk_id']: c for c in chunks}\n",
        "print('Unique chunk ids:', len(chunk_by_id))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7733845710754395 emx:visual_extraction:6a55e73ff2d9 page 0\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby ho...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# Embedding-backed search; falls back to TF-IDF\n",
        "\n",
        "def search_embedded(query: str, k: int = 3):\n",
        "\tif E is None:\n",
        "\t\tprint('Embeddings not loaded; falling back to TF-IDF search()')\n",
        "\t\treturn search(query, k)\n",
        "\tfrom fastembed import TextEmbedding\n",
        "\temb = TextEmbedding('BAAI/bge-small-en-v1.5')\n",
        "\tqv = np.array(list(emb.embed([query]))[0], dtype=np.float32)\n",
        "\t# cosine similarity\n",
        "\tqv = qv / (np.linalg.norm(qv) + 1e-9)\n",
        "\tEv = E / (np.linalg.norm(E, axis=1, keepdims=True) + 1e-9)\n",
        "\tscores = Ev @ qv\n",
        "\tidxs = scores.argsort()[-k:][::-1]\n",
        "\tresults = []\n",
        "\tfor idx in idxs:\n",
        "\t\tchunk_id = emb_records[idx]['chunk_id']\n",
        "\t\tc = next(c for c in chunks if c['chunk_id'] == chunk_id)\n",
        "\t\tresults.append({\n",
        "\t\t\t'score': float(scores[idx]),\n",
        "\t\t\t'document_id': c['parent_document_id'],\n",
        "\t\t\t'page_num': c['page_num'],\n",
        "\t\t\t'section_path': c['section_path'],\n",
        "\t\t\t'snippet': c['text'][:240] + ('...' if len(c['text'])>240 else '')\n",
        "\t\t})\n",
        "\treturn results\n",
        "\n",
        "# Demo\n",
        "for r in search_embedded('energy savings tips'):\n",
        "\tprint(r['score'], r['document_id'], f\"page {r['page_num']}\")\n",
        "\tprint(r['snippet'])\n",
        "\tprint('---')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.4048466114956216 emx:visual_extraction:6a55e73ff2d9 page 1\n",
            "Your top three tailored energy-saving tips Caulk windows and doors Upgrade your refrigerator Adjust thermostat settings Save money and energy Look for an Energy Star label Biggest energy saving option One of the biggest Older model Set your...\n",
            "---\n",
            "0.12844813733286536 emx:visual_extraction:6a55e73ff2d9 page 0\n",
            "Home Energy Report: electricity March report Account number: 954137 Service address: 1627 Tulip Lane Dear JILL DOE, here is your usage analysis for March. Your electric use: 18% more than similar nearby homes You TT A bove Similar nearby ho...\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "def search(query: str, k: int = 3):\n",
        "\tqv = vectorizer.transform([query])\n",
        "\tscores = cosine_similarity(qv, X)[0]\n",
        "\ttopk = scores.argsort()[::-1][:k]\n",
        "\tresults = []\n",
        "\tfor idx in topk:\n",
        "\t\tc = chunks[idx]\n",
        "\t\tresults.append({\n",
        "\t\t\t'score': float(scores[idx]),\n",
        "\t\t\t'document_id': c['parent_document_id'],\n",
        "\t\t\t'page_num': c['page_num'],\n",
        "\t\t\t'section_path': c['section_path'],\n",
        "\t\t\t'snippet': c['text'][:240] + ('...' if len(c['text'])>240 else '')\n",
        "\t\t})\n",
        "\treturn results\n",
        "\n",
        "for r in search('energy savings tips'):\n",
        "\tprint(r['score'], r['document_id'], f\"page {r['page_num']}\")\n",
        "\tprint(r['snippet'])\n",
        "\tprint('---')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Ingest current run into Postgres (Phase 5)\n",
        "This cell applies the migration and ingests documents, chunks, and embeddings into Postgres using `DATABASE_URL`. Requires pgvector extension installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing missing packages: ['psycopg', 'dotenv']\n",
            "DATABASE_URL not set; skipping ingestion\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Ingest to Postgres using DATABASE_URL\n",
        "import os\n",
        "from importlib import util as _iu\n",
        "\n",
        "# Ensure psycopg and dotenv available in kernel\n",
        "missing = []\n",
        "for m in ('psycopg', 'dotenv'):\n",
        "\tif _iu.find_spec(m) is None:\n",
        "\t\tmissing.append(m)\n",
        "if missing:\n",
        "\tprint('Installing missing packages:', missing)\n",
        "\t_ = subprocess.run([sys.executable, '-m', 'pip', 'install', *missing, '--quiet'], text=True)\n",
        "\n",
        "# Apply migration (requires psql available). Skip if not present.\n",
        "DATABASE_URL = os.environ.get('DATABASE_URL') or os.environ.get('POSTGRES_DSN')\n",
        "if DATABASE_URL:\n",
        "\tprint('Using DATABASE_URL')\n",
        "\t# Best effort migration via psql if available\n",
        "\tpsql = subprocess.run(['which', 'psql'], capture_output=True, text=True)\n",
        "\tif psql.returncode == 0:\n",
        "\t\tprint('Applying migration 001_init.sql')\n",
        "\t\t_ = subprocess.run(['psql', DATABASE_URL, '-f', str(project_root / 'db' / 'migrations' / '001_init.sql')], text=True)\n",
        "\telse:\n",
        "\t\tprint('psql not found; please apply db/migrations/001_init.sql manually')\n",
        "\t# Ingest\n",
        "\tprint('Ingesting current run...')\n",
        "\tret = subprocess.run([sys.executable, str(project_root / 'scripts' / 'ingest_to_postgres.py')], capture_output=True, text=True)\n",
        "\tprint(ret.stdout or ret.stderr)\n",
        "else:\n",
        "\tprint('DATABASE_URL not set; skipping ingestion')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pdf-extractor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
